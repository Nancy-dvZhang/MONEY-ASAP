---
title: "BST260 Final Project: A$AP MONEY"
authors: Octavia Huang, Yidan Ma, Hainan Xiong, Kexin Yang, Diwei Zhang
output:
  html_document:
    df_print: paged
---
## Overview and Motivation

As a group of aspiring data scientists, we are highly interested and motivated to embark on our career in tech companies. Before seeking a full-time job, we want to learn more about the salaries from different companies. While some of us have started on our internships already, the rest would like to also navigate the status quo of data science and STEM career options as possible alternative plans. We would also like to know how the representativeness of certain socioeconomic groups affect their salary negotiation, and whether our project could help with uncovering the transparency lacking in this dynamic.

We would like to explore more about income and realize our own placements from analysis of the data.  Specifically, we analyzed a set of factors that are potentially predictive of the outcome of us earning $100k/yr upon graduation. We will be using a selection of methods to analyze and visualize the data as well as reduce the dimension before applying logistic regression, K-nearest neighbor, decision tree, and random forest to make predictions. The data would also be presented in a shiny app embedded in the website to allow public access. Apart from our own money concerns. 

Our preferred features include: company, level, title, education level, years of experience, and some other personal backgrounds.

## Related Work

Our project is inspired by an article written by Kate Birch at Business Chief about the rising salary of tech companies in the United States(https://businesschief.com/technology-and-ai/tech-salaries-us-continue-rise-report-dice). Even under the impact of the COVID-19 pandemic, where pay increases have halted for other economic sectors, tech job salaries still rise by 3.6% in the year of 2020. Among those tech jobs, cybersecurity analysts and data scientists witnessed the biggest increase in their salaries: 16.3% for cyber security analysts and 12.8% for data scientists. Average salaries for tech jobs, at different locations, vary from each other. However, all of them are above 100k. As aspiring health data science students at Harvard Chan, we want to land a job as soon as possible. And in the meantime we do want to evaluate our probability of landing a job with a salary higher than 100k per year. Thatâ€™s how we started our project.

We are also inspired by the website www.levels.fyi, which is the website that provides the dataset we are using for this project. Levels.fyi is a website that provides salary information for the technology community. It allows users to share their salary information and serves as a large accessible online database where others can easily search for salary information based on certain criteria such as company, title, etc. This website mainly functions as a search database, and it also offers a few visualizations about salary information for top companies. 

Inspired by these functions, we think it would also be a good idea to build a salary predictor, which would make it even easier for people to estimate their potential salary based on several key features. So we want to build a salary predictor that only requires the user to select their feature and automatically output their predicted salary, as well as a confidence interval for this salary, for each typical job titles in the tech industry. This way, instead of manually filtering out all the feature criteria, selecting the region and going through tens and hundreds of salary inputs as they would have on the www.levels.fyi website, people can can a clear idea of the typical salary range in a table, which we believe is much more distinctive and user-friendly. Besides, we also decided to add more visualizations for the salary data to make it easier for people to evaluate the salary distribution within the tech industry.

## Initial Questions

The initial question of our project is to predict the binary outcome of earning a base salary of $100k per year given a set of features including location, company, title, education level, experience, and other personal backgrounds using the Kaggle Data Science and STEM income dataset which will be introduced in the **Data** section. To answer this question, we used many models including logistic regression, K-nearest neighbor, decision tree, random forest, and linear regression. Through the process, we used these models to make predictions and compared their accuracies. As a result, all of our prediction models can make sensible predictions with relatively high sensitivity and specificity; the random forest model reached the highest accuracy of 0.9085. Visualization of results were done using tSNE to facilitate interpretation and a Shiny salary predictor was implemented to allow public access. 

While analyzing data to answer our main study question, we had many interesting sub-findings. For instance, among all job titles present in this dataset, business analyst has the lowest mean base salary and software engineering manager has the highest. In addition, sales, human resources, and recruiter have a larger variation in their base salaries than titles like data scientist and software engineer. We also find out that 5 years of working experience is a clear cutoff for our prediction of base salary. Besides our initial question, we also become interested in exploring how the representativeness of certain socioeconomic groups affect their salary negotiation. We investigated the significance of gender and race of individuals in base salary prediction. Aspiringly, gender is not a significant factor of base salary prediction in tech companies in this dataset. Using a significance level cutoff of p < 0.001, all races are not significant factors in base salary prediction comparing with race Asian except race white, which has an odds ratio of 1.34.

## Data

Kaggle Data Science and STEM income dataset (https://www.kaggle.com/jackogozaly/data-science-and-stem-salaries)
This is a dataset scraped from the website of levels.fyi, a site with a visualised, crowdsourced database for transparent leveling charts across companies to help with job negotiation and possibly prevent underpaying for those underrepresented within tech industry.

## Exploratory Analysis

```{r}
library(tidyverse)
library(ggplot2)
#library(matrixStats)
library(dplyr)
library(stringr)
#install.packages("ggcorrplot")
#library(ggcorrplot)
#install.packages("splitstackshape")
library(splitstackshape)
#install.packages("caret")
library(caret)
#install.packages("e1071")
library(e1071)
#install.packages("pROC")
library(pROC)
#install.packages("rpart")
#install.packages("rpart.plot")
library(rpart)
library(rpart.plot)
#install.packages("randomForest")
library(randomForest)
#install.packages("Rtsne")
library(Rtsne)
```

```{r}
dataframe = read.csv("Levels_Fyi_Salary_Data.csv")
head(dataframe)
```

```{r}
summary(dataframe)
```

```{r}
table(dataframe$Race)
```

```{r}
table(dataframe$Education)
```

```{r}
filtered_df <- dataframe %>% select(c("company", "title", "totalyearlycompensation", "location", "yearsofexperience", "yearsatcompany", 
'basesalary','stockgrantvalue','bonus','gender','cityid','dmaid','Race','Education'))

```

```{r}
filtered_df$company <- as.factor(filtered_df$company)
filtered_df$title <- as.factor(filtered_df$title)
filtered_df$Race <- as.factor(filtered_df$Race)
filtered_df$Education <- as.factor(filtered_df$Education)
```

```{r}
#table(filtered_df$company)
length(unique(filtered_df$company))
```

```{r}
length(unique(filtered_df$title))
```

```{r}
filtered_df %>% 
  ggplot(aes(title,basesalary))  + geom_boxplot() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ylim(0,400000)
```

```{r}
filtered_df %>% filter(!is.na(Race)) %>%
  ggplot(aes(Race,basesalary))  + geom_boxplot() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ylim(0,400000)
```

```{r}
sum(is.na(filtered_df$Race)) / length(filtered_df$Race)
```

```{r}
filtered_df %>% filter(!is.na(Education)) %>%
  ggplot(aes(Education,basesalary))  + geom_boxplot() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ylim(0,400000)
```

```{r}
sum(is.na(filtered_df$Education)) / length(filtered_df$Education)
```

```{r}
filtered_df$y <- ifelse(filtered_df$basesalary>100000, 1, 0)
table(filtered_df$y)
```

```{r}
df <- filtered_df %>% select(c("company", "title", "location", "yearsofexperience", "yearsatcompany", 
'basesalary','gender','cityid','dmaid','Race','Education'))
```

## Final Analysis

### Logistic Regression

Company and location information is excluded from our multinomial logistic regression model because they are categorical data with so many categories that they obscure our results. First, we fit a full model with all covariates we selected to see the general relationship pattern. We excluded the individuals with NA entries in either Education, Race, and gender to tidy up the data and to make trends more visible. Another reason we decide to drop these entries is that without them, we still have enough data (>20,000) to draw strong prediction power. 

```{r}
df_1 <- filtered_df %>% select(c("title", "yearsofexperience", "yearsatcompany",'gender','cityid','dmaid','Race','Education', 'y')) %>% filter(!is.na(Education) & !is.na(gender) & !is.na(Race))
mod_logistic_whole = glm(y ~ ., data = df_1, family=binomial())
summary(mod_logistic_whole)
```

From our full model, we see that all of our covariates provide helpful information with as least one category being significant except gender. We then try a bidirectional step-wise selection procedure to further feature selection.

```{r}
mod_logistic_step <- step(glm(y ~ 1, data = df_1), ~ title + yearsofexperience + yearsatcompany + gender + cityid + dmaid + Race + Education, direction = "both")
summary(mod_logistic_step)
```

The step-wise selection also shows gender to be insignificant, so drop gender. Also, drop entries with 0 in basesalary, they are missing completely at random.

```{r}
df_2 <- filtered_df %>% select(c("basesalary", "title", "yearsofexperience", "yearsatcompany",'cityid','dmaid','Race','Education', 'y')) %>% filter(!is.na(Education)) %>% filter(!is.na(Race)) %>% filter(basesalary!=0)

df_2 <- df_2 %>% select(c("title", "yearsofexperience", "yearsatcompany",'cityid','dmaid','Race','Education', 'y'))
df_2$y <- as.factor(df_2$y)
dim(df_2)
```

Build our final logistic model with the edited dataset.

```{r}
mod_logistic = glm(y ~ ., data = df_2, family=binomial())
summary(mod_logistic)
```

Feature selection done, test accuracy next.

```{r}
set.seed(1)
x <- stratified(df_2, "y", 0.8, keep.rownames = TRUE)
train_set <- x %>% dplyr::select(-rn)
train_index <- as.numeric(x$rn)
test_set <- df_2[-train_index,]
print(dim(train_set))
print(dim(test_set))
head(train_set)
head(test_set)
```

```{r}
mod_logistic_train = glm(y ~ ., data = train_set, family=binomial())
preds_hat <- predict(mod_logistic_train, newdata = test_set, type= "response")
preds <- ifelse(preds_hat>0.5,1,0)
preds <- as.numeric(substring(preds,-1))
confusionMatrix(data = as.factor(preds), reference = as.factor(test_set$y), positive = "1")
```

Our logistic regression model yields an overall accuracy of 0.863 with 95% CI (0.8524, 0.8731) and a very small p-value, which indicates that we did an OK job in predicting basesalary >= 100K. However, our sensitivity is 0.9377 and our specificity is 0.6453, giving a balanced accuracy of 0.7915. This means our logistic regression prediction model is making more Type 1 error (False positive) than type 2 errors.


### Data Manipulation for KNN, Decision Tree, and Random Forest
We want to take a look on how many unique values are in our dataset, for categorical feature such as company and feature.
```{r}
filtered_dataset <- filtered_df %>% filter(!is.na(.))
print(length(unique(filtered_dataset$company)))
print(length(unique(filtered_dataset$title)))

```
Since there are too many unique company in our dataset, it is better to cluster these companies to reduce the dimensionality of the company feature. Our current approach is to cluster companies based on its size(a.k.a number of time each company appear in the dataset). We re-classify company into 5 classes. 
```{r}
sorted_company <- filtered_dataset %>% count(tolower(company)) %>% arrange(desc(n))
sorted_company
```

```{r}
filtered_dataset <- filtered_dataset %>% mutate(company = tolower(company))
df <- left_join(filtered_dataset, sorted_company, by = c("company" = "tolower(company)"))
df <- df %>% mutate(size_category  = case_when(
  n <= 10 ~ "<10",
  n > 10 & n <= 50 ~ "10~50",
  n > 50 & n <= 100 ~"50~100",
  n > 100 & n <= 500 ~ "100~500",
  n > 500 ~ "500+"

))
df <- df %>% mutate(employee_num = n) %>% select(-n)
head(df)
```
The location feature looks a little bit untidy. So we separate it into city, state, country. 
```{r}
location <- df %>%
  count(location) %>%
  arrange(desc(n)) %>%
  mutate(place = location) %>%
  separate(place, sep = ",",  c("city", "state", "country"))

location[is.na(location)] <- "US"
df <- left_join(df, location, by = "location") %>% select(-location)
head(df)
```

After the manipulation we want to see how exactly the distribution of dataset looks like for different locations
```{r}
df %>% ggplot(aes(size_category, basesalary)) + geom_boxplot()
```
```{r}
df %>% ggplot(aes(country, basesalary)) + geom_boxplot()
```
We want to take a look on the U.S. data specifically
```{r}
df %>% filter(country == "US") %>% ggplot(aes(state, basesalary)) + geom_boxplot()
```
we want to drop missing value and see how the dataset looks like.
```{r}
cleaned_df <- drop_na(df)
dim(cleaned_df)
```
We also need to factorize our variables. 
```{r}
cleaned_df <- cleaned_df %>% mutate(gender = as.factor(gender), size_category = as.factor(size_category), 
                                  state = as.factor(state), country = as.factor(country), y = as.factor(y))
dataset <- cleaned_df %>% select(c("title", "yearsofexperience", "yearsatcompany","gender","cityid","dmaid",
                                  "Race","Education", "y","size_category","state","country"))
sapply(dataset, "class")
```
Now our dataset looks good and we can get started to build up our model!


### KNN
```{r}
set.seed(42)
x <- stratified(dataset, "y", 0.8, keep.rownames = TRUE)
train_set <- x %>% dplyr::select(-rn)
train_index <- as.numeric(x$rn)
test_set <- dataset[-train_index,]
print(dim(train_set))
print(dim(test_set))
```

```{r}
set.seed(42)
knn_fit <- train_set %>% knn3(y~., data = ., k = 12)
knn_hat <- predict(knn_fit, newdata = test_set, type = "class")
knn_p <- f_hat2 <- predict(knn_fit, newdata = test_set, k=12)[,2]
confusionMatrix(data = knn_hat, reference = as.factor(test_set$y), positive = "1")
```

## Decision Tree
  
We then apply a decision tree to visualize the prediction of the majority class of our outcome (whether one receives >$100k/yr base salary) based within the decision process partitioning based on a combination of variables.
  
We selected a set of covariates that could potentially be predictive of the outcome variable. After combinatory attempts to construct the tree, three combinations resulted in clear partition of majority class in the outcome variable:
  
```{r}
# Create dataset for the trees
dataset_dt <- cleaned_df %>% select(c("title", "yearsofexperience", "yearsatcompany","gender","cityid","dmaid",
                                  "Race","Education", "y","size_category","state","country","employee_num"))
sapply(dataset_dt, class)

# Decision Tree using cityid (proxy for location), years of experience, and years at company
dt_df1 <- dataset_dt %>% dplyr::select(., y, cityid, yearsofexperience, yearsatcompany)
fit1 <- rpart(as.factor(y) ~ ., data = dt_df1)

rpart.plot(fit1)
```
  
In the first tree, the outcome is partitioned by years of experience < 11, city id as a proxy for the importance of locations, and years of experience < 5.
  
```{r}
# Next, Decision Tree using years of experience, and employee number
dt_df2 <- dataset_dt %>% dplyr::select(.,y, yearsofexperience, employee_num)
fit2 <- rpart(as.factor(y) ~ ., data = dt_df2)

rpart.plot(fit2)
```
  
In the second decision tree, if years of experience > 5, we predict the outcome to be >$100k/yr base salary. If not, if employee number of the company is larger than 506, the outcome category is also predicted to be >\$100k/yr base salary. The next divisions are made by years of experience > 3 and emplyee number > 136.
  
```{r}
# Thirdly, Decision Tree using title, company size category, and years of experience
dt_df3 <- dataset_dt %>% dplyr::select(.,y, title, size_category, yearsofexperience)
fit3 <- rpart(as.factor(y) ~ ., data = dt_df3)

rpart.plot(fit3)
```
  
The third tree predicts the outcome by the same cutoffs in years of experience. The rest of the division lies in size category and title.
  
  
### Random Forest

We build a random forest model with 3 features for each tree (~ sqrt level)
```{r}
set.seed(42)
rf_fit <- randomForest(y ~ .-state-country, data = train_set, mtry = 3)
rf_hat <- predict(rf_fit, newdata = test_set, type = "class")
rf_p <- predict(rf_fit, newdata = test_set, type = "prob")[,2]
confusionMatrix(data = rf_hat, reference = as.factor(test_set$y), positive = "1")
```

Now, check the feature importance

```{r}
rf_im <- importance(rf_fit)
as.matrix(rf_im[order(-rf_im[,1]),])
```


### Visualization

Here, we do a visualization using tSNE.

First, to reduce computational burden, we randomly sample 5000 points for visualization. At the same time, since `cityid` and `dmaid` are two strongest factors, we only keep `dmaid` to avoid overfitting.

```{r}
data_sample0 <- dataset[sample(1:nrow(dataset), size = 5000),]
data_sample <- data_sample0[ , -which(colnames(data_sample0) %in% c("cityid"))]
```

```{r}
# Obtain the value of 2-D tSNE embedding
tsne_out <- Rtsne(
 data_sample,
 dims = 2,
 pca = TRUE,
 perplexity = 100,
 check_duplicates = FALSE
)
```

Now, let's see the pattern of different labels

```{r}
rd_rs <- as.data.frame(tsne_out$Y)
rd_rs$Class<- data_sample$y
# first obtain the color
length(unique(rd_rs$Class))
mainPalette <- rainbow(9)
ggplot(rd_rs, aes(x=V1, y=V2, color=Class)) +
  geom_point(size=1.25) +
  labs(title = "t-SNE",
       x = "t-SNE1",
       y = "t-SNE2") +
  theme(plot.title = element_text(hjust = 0.5))
```

Let's see the pattern of the strongest factor `dmaid`

```{r}
rd_rs <- as.data.frame(tsne_out$Y)
rd_rs$Class<- data_sample$dmaid
# first obtain the color
length(unique(rd_rs$Class))
mainPalette <- rainbow(9)
ggplot(rd_rs, aes(x=V1, y=V2, color=Class)) +
  geom_point(size=1.25) +
  labs(title = "t-SNE",
       x = "t-SNE1",
       y = "t-SNE2") +
  theme(plot.title = element_text(hjust = 0.5))
```


### Linear Regression

We also wanted to fit the Dataset into a linear regression model so that we will be able to make predictions on base salary given information about a person's relevant features. So in this analysis, we explored the performance of different feature engineering and encoding methods, as well as the prediction performance of several linear regression model. We will choose the best model to make our base salary predictor in the next section.


#### Explore Dataset

```{r}
dataframe = read.csv("Levels_Fyi_Salary_Data.csv")
head(dataframe)
```

```{r}
filtered_df <- dataframe %>% 
  select(c("company", "title", "totalyearlycompensation", "location", "yearsofexperience", "yearsatcompany", 
'basesalary','stockgrantvalue','bonus','gender','cityid','dmaid','Race','Education')) %>% 
  mutate(company = as.factor(company), title <- as.factor(title), Race <- as.factor(Race), Education <- as.factor(Education)) %>%
  filter(!is.na(Education) & !is.na(gender) & !is.na(Race))

head(filtered_df)
```

Since the location in this data frame is encoded as City, State, Country (except for locations in the US, where it is encoded as City, State), we would like to split this into 3 separate features and take a look into the distribution of countries. 

```{r}
location <- filtered_df %>%
  count(location) %>%
  arrange(desc(n)) %>%
  mutate(place = location) %>%
  separate(place, sep = ",",  c("city", "state", "country"))

location[is.na(location)] = "US"
```

Join the location table with our original dataset.

```{r}
joined_df <- filtered_df %>%
  left_join(location, by = "location")

head(joined_df)
```

After ploting the base salary for different countries, we can see that there is a large discrapencies between each countries.

```{r}
joined_df %>% 
  ggplot(aes(country,basesalary))  + 
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ylim(0,400000)
```

If we list the number of data we have for each countries, we can also see that the dataset is very imbalanced -- the majoriety of data comes from the US, where as other countries only contribute a very small amount of data, this can be a major problem for machine learning models, since there may not be enough data for the model to learn the representation for other countries properly. Therefore, we decided to filter only data from the US, and train the linear regression model as a base salary predictor for the US.

```{r}
joined_df %>%
  count(country) %>%
  arrange(desc(n))
```

```{r}
us_df <- joined_df %>%
  filter(country == "US")

head(us_df)
```


#### Feature Engineering

Next we would like to proceed to the feature engineering stage. Since we are building the linear regression model to use it as the base salary predictor for our Shiny App, we can only include features that we want users to input for their salary prediction. This means that we cannot use feature such as `dmaid` in the model since we don't know what this feature represent. So after looking at our EDA plots, we decided to choose 6 features: `title`, `company`, `yearsatcompany`, `yearsofexperience`, `Education`, and `Race` as our features. 

Considering the ease of calculation for our Shiny App Predictor, we also decided to use a categorical encoding for each feature instead of one-hot encoding, even though empirically one-hot encoding may yield better results. 

To do the feature engineering for `title`, we plot the average base salary for each title again. We can see that there is a clear difference in average base salary for different title.

```{r}
us_df %>% 
  drop_na() %>%
  mutate(company = str_to_title(company)) %>%
  select(c("company", "title", "location", "yearsofexperience", "yearsatcompany", 
'basesalary','gender','cityid','dmaid','Race','Education'))

```

```{r}
us_df %>% 
  ggplot(aes(title,basesalary))  + 
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ylim(0,400000)
```

In order to make sure that our title encoding can capture this pattern of differences, we decided to first encode each title category as their average base salary, and later we would also normalize this encoding value to range between [0, 1].

```{r}
title_mean <- us_df %>%
  group_by(title) %>%
  summarise(title_encode = mean(basesalary)/10000) %>%
  arrange(desc(title_encode))

title_mean
```

We repeat the same process for the other 5 featues.

```{r}
us_df %>% 
  filter(!is.na(Race)) %>%
  ggplot(aes(Race,basesalary))  + 
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ylim(0,400000)
```

```{r}
race_mean <- us_df %>%
  group_by(Race) %>%
  summarise(race_encode = mean(basesalary)/10000) %>%
  arrange(desc(race_encode))

race_mean
```

```{r}
us_df %>% 
  filter(!is.na(Education)) %>%
  ggplot(aes(Education,basesalary))  + 
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ylim(0,400000)
```

```{r}
education_mean <- us_df %>%
  group_by(Education) %>%
  summarise(education_encode = mean(basesalary)/10000) %>%
  arrange(desc(education_encode))

education_mean
```

```{r}
us_df %>% 
  filter(!is.na(yearsofexperience)) %>%
  ggplot(aes(as.factor(yearsofexperience),basesalary))  + 
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ylim(0,400000)
```

```{r}
experience_mean <- us_df %>%
  group_by(yearsofexperience) %>%
  summarise(experience_encode = mean(basesalary)/10000) %>%
  arrange(desc(experience_encode))

experience_mean
```

```{r}
us_df %>% 
  filter(!is.na(yearsatcompany)) %>%
  ggplot(aes(as.factor(yearsatcompany),basesalary))  + 
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ylim(0,400000)
```

```{r}
yearatcompany_mean <- us_df %>%
  group_by(yearsatcompany) %>%
  summarise(yearatcompany_encode = mean(basesalary)/10000) %>%
  arrange(desc(yearatcompany_encode))

yearatcompany_mean
```

For the feature `company`, we know that this dataset includes data from 1633 distinct companies, so we think it would probably be better it we convert `company` into new new feature with less levels. To do this, we first take a look into how the `company` feature distribution looks like.

```{r}
company <- us_df %>%
  count(company) %>%
  arrange(desc(n))

head(company)
```

We also plot this distribution in a histrogram.

```{r}
company %>%
  filter(n <= 100) %>%
  ggplot(aes(n)) +
  geom_histogram(bins = 20)
```

After taking a look into the specific data, we see that the number of appearance a company has in this dataset, `n`, actually highly correlates with the actual company size. For instance, the major tech companies like Amazon, Micorsoft, Google, Facebook, and Apple ranks top 5 in the tabke above, and all has over 1000 data entries. So we decided to use this `n` create a new feature called `company_size` and use this feature for our model training. 

```{r}
company_size <- company %>%
  mutate(company_size = case_when(
    n <= 3 ~ "1",
    n <= 10 ~ "3 - 10",
    n <= 50 ~ "11 - 50",
    n <= 100 ~ "51 - 100",
    n <= 500 ~ "101 - 500",
    TRUE ~ ">500"
  ))
```

```{r}
us_df <- us_df %>%
  left_join(company_size, by = "company")

head(us_df)
```

We also repeat the same process for feature engineering.

```{r}
us_df %>% 
  filter(!is.na(company_size)) %>%
  ggplot(aes(company_size,basesalary))  + 
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ylim(0,400000)
```

```{r}
company_mean <- us_df %>%
  group_by(company_size) %>%
  summarise(company_encode = mean(basesalary)/10000) %>%
  arrange(desc(company_encode))

company_mean
```

To create the final dataframe for our model, we need to join all encoded feature together and normalize these feature to range between [0, 1] using the `min_max_norm` function.

```{r}
min_max_norm <- function(x) {
    (x - min(x)) / (max(x) - min(x))
  }
```

```{r}
lm_us_df<- us_df %>%
  left_join(company_mean, by = "company_size") %>%
  left_join(title_mean, by = "title") %>%
  left_join(education_mean, by = "Education") %>%
  left_join(race_mean, by = "Race") %>%
  left_join(experience_mean, by = "yearsofexperience") %>%
  left_join(yearatcompany_mean, by = "yearsatcompany") %>%
  mutate(company = min_max_norm(company_encode), title = min_max_norm(title_encode), education = min_max_norm(education_encode), race = min_max_norm(race_encode), experience = min_max_norm(experience_encode), yearsatcompany = min_max_norm(yearatcompany_encode)) %>%
  select(basesalary, company, title, education, race, experience, yearsatcompany)

head(lm_us_df)
```


#### Saving cleaned data and encoded features

We save the cleaned dataframe into a .csv file and also save all the encoding data so that we can use them for reference when we later calculate the predicted base salary in our Shiny App Predictor.

```{r}
write.csv(x=us_df, file="project/clean_us_salary_data.csv", row.names = FALSE)
```

```{r}
company_encode <- company_size %>%
  left_join(company_mean, by = "company_size") %>%
  mutate(x = company, y = min_max_norm(company_encode)) %>%
  select(x, y, company_size)

write.csv(x=company_encode, file="project/company_encode.csv", row.names = FALSE)

head(company_encode)
```

```{r}
education_encode <- education_mean %>%
  mutate(x = Education, y = min_max_norm(education_encode)) %>%
  select(x, y)

write.csv(x=education_encode, file="project/education_encode.csv", row.names = FALSE)

head(education_encode)
```

```{r}
title_encode <- title_mean %>%
  mutate(x = title, y = min_max_norm(title_encode)) %>%
  select(x, y)

write.csv(x=title_encode, file="project/title_encode.csv", row.names = FALSE)

head(title_encode)
```

```{r}
yearsatcompany_encode <- yearatcompany_mean %>%
  mutate(x = yearsatcompany, y = min_max_norm(yearatcompany_encode)) %>%
  select(x, y)

write.csv(x=yearsatcompany_encode, file="project/yearsatcompany_encode.csv", row.names = FALSE)

head(yearsatcompany_encode)
```

```{r}
race_encode <- race_mean %>%
  mutate(x = Race, y = min_max_norm(race_encode)) %>%
  select(x, y)

write.csv(x=race_encode, file="project/race_encode.csv", row.names = FALSE)

head(race_encode)
```

```{r}
experience_encode <- experience_mean %>%
  mutate(x = yearsofexperience, y = min_max_norm(experience_encode)) %>%
  select(x, y)

write.csv(x=experience_encode, file="project/experience_encode.csv", row.names = FALSE)

head(experience_encode)
```


#### Training the Linear Regression model

To train our linear regression model, we first need to split the dataset into training and testing.

```{r}
set.seed(1)
idx <- sample.int(n = nrow(lm_us_df), size = floor(0.8 * nrow(lm_us_df)), replace = FALSE)

train <- lm_us_df[idx, ]
test <- lm_us_df[-idx, ]

dim(train)
dim(test)
```

Fit the training dataset into the linear regression model.

```{r}
lm1 <- lm(basesalary ~ ., data = train)
summary(lm1)
```

We can see that all the features are significant, this suggests that they are all important and contribute to the final prediction of base salary. So we now save these coefficient for our base salary predictor.

```{r}
lm_coef <- summary(lm1)$coefficients
lm_coef_df <- data.frame(coef = rownames(lm_coef), value = lm_coef[, 1], std = lm_coef[, 2], lower = lm_coef[, 1] - lm_coef[, 2], upper = lm_coef[, 1] + lm_coef[, 2])
write.csv(lm_coef_df, file = "project/lm_coef.csv", row.names = FALSE)
```

To evaluate model performance, we choose to calculate its RMSE.

```{r}
RMSE <- function(true_ratings, predicted_ratings){
    sqrt(mean((true_ratings - predicted_ratings)^2))
}
```

The RMSE for the test dataset prediction is relatively large, however, considering the noise that naturally exists in this dataset, where people with the exact same features is likely to have different base salary, a large testing RMSE doesn't necessarily suggest that our model is performing bad, and we would need further analysis to fairly evaluate our model performance.

```{r}
lm_pred <- predict(lm1, newdata = test)
lm_rmse <- RMSE(lm_pred, test$basesalary)
lm_rmse
```

By comparing our model prediction and the actual base salary in the testing dataset, we can see that while not accurate, our model is able to capture the general trend of base salary level. This means that the prediction of out model can at least provide some reference for a person's potential base salary based on their input.

```{r}
lm_pred[1:10]
test$basesalary[1:10]
```


#### Optimizing Model Performance - One-hot Encoding and Regularization

To better evaluate our previous model performance, we also want to explore whether we can optimize the previous simple Linear Regression model through one-hot encoding and regularization. To do this, we first need to prepare the one-hot encoded dataset.

```{r}
library(caret)

df_one_hot <- us_df %>%
  select(title, yearsofexperience, yearsatcompany, basesalary, Education, company_size)

dmy <- dummyVars(" ~ .", data = df_one_hot)
df_one_hot <- data.frame(predict(dmy, newdata = df_one_hot))
str(df_one_hot)
```

Split training and testing dataset.

```{r}
train_one_hot <- df_one_hot[idx, ]
test_one_hot <- df_one_hot[-idx, ]

dim(train_one_hot)
dim(test_one_hot)
```

Fit a linear regression model using the one-hot encoded dataset.

```{r}
lm2 <- lm(basesalary ~ ., data = train_one_hot)
summary(lm2)
```

Evaluting model performance using RMSE on testing dataset. The RMSE is slightly lower than our previous linear regression model, which suggests that one-hot encoding do yield better results. However, since the difference is pretty small, we would still use the weight from our previous model for the ease of calculation.

```{r}
lm2_pred <- predict(lm2, newdata = test_one_hot)
lm_rmse <- RMSE(lm2_pred, test_one_hot$basesalary)
lm_rmse
```

To explore the effect of regularization, we also fit the one-hot encoded data into a ridge regression model.

```{r}
library(glmnet)

x = as.matrix(subset(train_one_hot, select=-c(basesalary)))
y_train = train_one_hot$basesalary
x_test = as.matrix(subset(test_one_hot, select=-c(basesalary)))
y_test = train_one_hot$basesalary

ridge_reg = glmnet(x, y_train, alpha = 1, family = 'gaussian', lambda = 0.8)
ridge_reg
```

The RMSE is higher for ridge regression, so we would not choose this model.

```{r}
pred <- predict(ridge_reg, newx = x_test)[, 1]
rmse_ridge <- RMSE(pred, y_test)
rmse_ridge
```

Overall, we think our first simple linear regression gives a reasonably good prediction on base salary, so we decided to use its weight to build our final salary predictor shiny app


### Shiny APP

We implemented 4 main features in our Shiny App, which are embedded in 4 seperate tabs.

#### Estimator

User can input their information for each of the 5 features in the drop-down bar, and our salary estimator will return a table of salary estimation as well as the confidence interval for each of the 15 titles.

#### Geo Map

User can either view the base salary distribution across different states in the United States, or choose to filter certain feature level and view the corresponding base salary distribution. However, there are missing value for certain states for specific feature levels.

#### Boxplot

User can view the boxplot of base salary distribution for each of the 6 features

#### Salary distribution plot

User can select a base salary upper bound, and view the distribution of salary.
 

```{r, echo=FALSE, warning=FALSE, include=FALSE}
library(shiny)
library(tidyverse)
library(ggplot2)
library(usmap)
library(ggplot2)
library(stringr)

```

```{r, echo=FALSE, warning=FALSE}

# library(rsconnect)
# rsconnect::deployApp('/Users/kexinyang/Desktop/BST 260/Project/Predictor.Rmd')

df <- read.csv("project/clean_us_salary_data.csv")
coef <- read.csv("project/lm_coef.csv")
race_encode <- read.csv("project/race_encode.csv")
company_encode <- read.csv("project/company_encode.csv")
education_encode <- read.csv("project/education_encode.csv")
title_encode <- read.csv("project/title_encode.csv")
experience_encode <- read.csv("project/experience_encode.csv")
yearsatcompany_encode <- read.csv("project/yearsatcompany_encode.csv")

ui <- fluidPage(
    
    theme = shinythemes::shinytheme("flatly"),

    titlePanel("Salary Estimator :)"),
    
    tabsetPanel(
        tabPanel("Estimator",
                 sidebarLayout(
                     sidebarPanel("Choose your features:", 
                        fluidRow(selectInput("race", label = "Select a race", selected = "Asian",
                                           choices = race_encode$x)),
                        fluidRow(selectInput("company", label = "Select a company", selected = "Amazon",
                                           choices = company_encode$x)),
                        fluidRow(selectInput("education", label = "Select a education",
                                           choices = education_encode$x)),
                        fluidRow(selectInput("yearsofexperience", label = "Select a years of experience", selected = "1",
                                           choices = experience_encode$x)),
                        fluidRow(selectInput("yearsatcompany", label = "Select a years at company", selected = "0",
                                           choices = yearsatcompany_encode$x))
                     ),
                     mainPanel(
                         tableOutput("estimated_salary")
                     )
                 ),
                 ),
        tabPanel("Geo Plot",
                 sidebarLayout(
                     sidebarPanel(
                         fluidRow(selectInput("title_g", label = "Select a title", selected = "All", choices = c("All", title_encode$x))),
                        fluidRow(selectInput("education_g", label = "Select a education", selected = "All", choices = c("All", education_encode$x))),
                        fluidRow(selectInput("experience_g", label = "Select a years of experience", selected = "All", choices = c("All", experience_encode$x))),
                        fluidRow(selectInput("years_g", label = "Select a years at company", selected = "All", choices = c("All", yearsatcompany_encode$x))
                     )
                ),
                 mainPanel(
                     plotOutput("geoplot")
                 )
            )
        ),
        tabPanel("BoxPlot",
                 fluidRow(
                     column(6, selectInput("x", label = "Select a feature to visualize",
                                         choices = as.list(c("Race", "title", "yearsofexperience", "yearsatcompany", "Education"))))
                 ),
                 fluidRow(
                     plotOutput("boxplot")
                 )
        ),
        tabPanel("Salary Distribution Plot",
                 fluidRow(
                     column(12, sliderInput("range", "Salary Range:", min = 10000, max = 2000000, value = 500000,
                                            step = 10000, sep = "", ticks = FALSE, animate = TRUE)
                     )
                 ),
                 fluidRow(
                     plotOutput("distributionplot")
                 )
        )
    )
    
)

server <- function(input, output) {
    
    output$estimated_salary = renderTable({
        
        salary_df <- data.frame("Title" = character(), "Salary Estimate" = numeric(), "Confidence interval" = character())
        for(i in 1:length(title_encode$x)) {
            title <- title_encode[i,]$x
            title_idx <- title_encode[i,]$y
        
            race_idx <- filter(race_encode, x == input$race)$y
            education_idx <- filter(education_encode, x == input$education)$y
            company_idx <- filter(company_encode, x == input$company)$y
            experience_idx <- filter(experience_encode, x == input$yearsofexperience)$y
            yearsatcompany_idx <- filter(yearsatcompany_encode, x == input$yearsatcompany)$y
            
            salary <- race_idx * coef[coef == "race", ]$value + company_idx * coef[coef == "company", ]$value + education_idx * coef[coef == "education", ]$value + title_idx * coef[coef == "title", ]$value + experience_idx * coef[coef == "experience", ]$value + yearsatcompany_idx * coef[coef == "yearsatcompany", ]$value + coef[coef == "(Intercept)", ]$value
            salary_lower <- race_idx * coef[coef == "race", ]$lower + company_idx * coef[coef == "company", ]$lower + education_idx * coef[coef == "education", ]$lower + title_idx * coef[coef == "title", ]$lower + experience_idx * coef[coef == "experience", ]$lower + yearsatcompany_idx * coef[coef == "yearsatcompany", ]$lower + coef[coef == "(Intercept)", ]$lower
            salary_upper <- race_idx * coef[coef == "race", ]$upper + company_idx * coef[coef == "company", ]$upper + education_idx * coef[coef == "education", ]$upper + title_idx * coef[coef == "title", ]$upper + experience_idx * coef[coef == "experience", ]$upper + yearsatcompany_idx * coef[coef == "yearsatcompany", ]$upper + coef[coef == "(Intercept)", ]$upper
            
            salary_df <- rbind(salary_df, c(title, round(salary), paste0(round(salary_lower), " - ", round(salary_upper))))
        }
        colnames(salary_df) <- c("Title", "Salary Estimate", "Confidence Interval")
        salary_df
    })
    
    output$boxplot = renderPlot({
        df$x <- df[, input$x]
        df %>% ggplot(aes(x = as.factor(x), y = basesalary)) +
            geom_boxplot() +
            xlab(sprintf("%s", input$x)) +
            ylab("Base salary") +
            ggtitle(sprintf("Distribution of base salary for %s", input$x)) +
            theme_minimal() +
            theme(plot.title=element_text(hjust=0.5),
                  axis.text.x = element_text(angle = 90))
    })
    
    output$geoplot = renderPlot({
        if (input$title_g != "All") {
            df <- filter(df, title == input$title_g)
        }      
        if (input$education_g != "All") {
            df <- filter(df, Education == input$education_g)
        }
        if (input$experience_g != "All") {
            df <- filter(df, yearsofexperience == input$experience_g)
        }
        if (input$years_g != "All") {
          df <- filter(df, yearsatcompany == input$years_g)
        }
        
        df$abbr <- str_replace_all(df$state, " ", "")
        us_map <- usmap::us_map()
        
        statename <- aggregate(cbind(x, y) ~ abbr, data=us_map, 
                            FUN=function(x)mean(range(x)))
        colnames(statename) <- c("abbr", "statex", "statey")
        
        map_df <- df %>%
              group_by(abbr) %>%
              summarise(avg_salary = mean(basesalary)) %>%
              mutate(salary_level = cut(avg_salary, breaks = c(0, 50000, 750000, 100000, 125000, 150000, 175000, 200000, 250000,300000, Inf), labels = c("50k", "750k", "100k", "125k","150k", "175k", "200k", "250k", "300k", ">300k"), right = TRUE)) %>%
              left_join(us_map, by = "abbr") %>%
              left_join(statename, by = "abbr")
        
        ggplot(data = map_df, aes(x = x, y = y, group = abbr, fill = salary_level)) + 
            geom_polygon(color = "grey36") +
            geom_text(aes(statex, statey, label = abbr), size=3) +
            theme(panel.grid.major = element_blank(), 
                  panel.background = element_blank(),
                  axis.title = element_blank(), 
                  axis.text = element_blank(),
                  axis.ticks = element_blank()) +
            scale_fill_brewer(palette = "Blues", name = "Salary Level") +
            theme(plot.title=element_text(hjust=0.5))
        })
    
    
        
        output$distributionplot = renderPlot({
            df %>%
                filter(basesalary <= as.numeric(input$range)) %>%
                ggplot(aes(basesalary)) +
                geom_histogram() +
                ggtitle(sprintf("Distribution for base salary in %s", input$country2)) +
                ylab(sprintf("Base salary")) +
                theme_minimal() +
                theme(plot.title=element_text(hjust=0.5))
        })
}

# Run the application 
shinyApp(ui = ui, server = server)

```
